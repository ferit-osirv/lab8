{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/ferit-osirv/lab8/blob/main/lab8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "ipXpivMK_PT8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8vtuqXEBP1j"
      },
      "source": [
        "# Lab 8 -  Single and multi-label classification with ResNets\n",
        "\n",
        "Ove laboratorijske vježbe se rješavaju u Google Colabu i spremaju na GitHub repozitorij koji je povezan na GitHub Classroom.\n",
        "\n",
        "## Kako riješiti zadatke?\n",
        "\n",
        "1. Prihvatite zadatak putem Google Classroom linka koji ćete dobiti. Google Classroom će kreirati repozitorij na vašem računu.\n",
        "2. Uđite u novokreiran repozitorij na vašem računu i kliknite na **.ipynb** datoteku, zatim kliknite **Open in Colab**.\n",
        "3. Zadatke rješavate u Google Colabu.\n",
        "\n",
        "## Kako spremiti (predati) zadatke?\n",
        "\n",
        "1. Unutar **Google Colaba** kliknite na **Open settings** kotačić u gornjem desnom kutu.\n",
        "2. Kliknite na **GitHub** tab i odaberite kvačicu za **Access private repositories and organizations**.\n",
        "3. Otvorit će se novi prozor da dodate pristup GitHubu. Kod **ferit-osirv** kliknite **Grant**.  \n",
        "4. Spremite i izađite iz postavki.\n",
        "\n",
        "\n",
        "5. Kliknite na **File > Save a copy in GitHub**.\n",
        "6. Odaberite kreiran repozitorij labosa **koji uključuje vaše ime**.\n",
        "\n",
        "> *Napomena:* Korake 1-4 morate napraviti samo prvi put."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction \n",
        "\n",
        "In this lab, we will build on top of the CNNs introduced in the previous (lab7) and explain to you the ResNet (residual network) architecture. Although CNNs perform well, the limited amount of annotated data requires the development of efficient and more complex, deeper network architectures. Deep convolutional neural networks have shown a significant increase in the accuracy for various segmentation and classification tasks. \n",
        "\n",
        "ResNet was introduced in 2015 by Kaiming He et al. in the article [\"Deep Residual Learning for Image Recognition\"](https://arxiv.org/abs/1512.03385) and is by far the most used model architecture nowadays. More recent developments in image models almost always use the same trick of residual connections, and most of the time, they are just a tweak of the original ResNet.\n",
        "\n",
        "We will first show you the basic ResNet as it was first designed, then explain to you what modern tweaks make it more performant. \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "teTCkV7Blesu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deep Residual Networks (ResNets)\n",
        "\n",
        "After the first CNN-based architecture (AlexNet) that win the ImageNet 2012 competition, every subsequent winning architecture uses more layers in a deep neural network to reduce the error rate. This works for less number of layers, but when we increase the number of layers, there is a common problem in deep learning associated with that called Vanishing/Exploding gradient. This causes the gradient to become 0 or too large. Thus when we increases number of layers, the training and test error rate also increases.\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://i.ibb.co/0n06TVs/resnet.png\">\n",
        "</p>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "In the above plot, we can observe that a 56-layer CNN gives more error rate on both training and testing dataset than a 20-layer CNN architecture, If this was the result of over fitting, then we should have lower training error in 56-layer CNN but then it also has higher training error. After analyzing more on error rate the authors were able to reach conclusion that it is caused by vanishing/exploding gradient (as the depth of CNN increases, information about the gradient passes through many layers, and it can vanish or accumulate large errors by the time it reaches the end of the network).\n",
        "\n"
      ],
      "metadata": {
        "id": "Nk-M0j7ToRmf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Residual Block\n",
        "\n",
        "In order to solve the problem of the vanishing/exploding gradient, this architecture introduced the concept called Residual Network. In this network we use a technique called skip connections . The skip connection skips training from a few layers and connects directly to the output.\n",
        "\n",
        "The approach behind this network is instead of layers learn the underlying mapping, we allow network fit the residual mapping. So, instead of say H(x), initial mapping, let the network fit, F(x)= H(x)–x which gives H(x)=F(x)+x.\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://i.ibb.co/cTkwsZt/residual-block.png\">\n",
        "</p>\n",
        "\n",
        "The advantage of adding this type of skip connection is because if any layer hurt the performance of architecture then it will be skipped by regularization. So, this results in training very deep neural network without the problems caused by vanishing/exploding gradient. \n",
        "\n"
      ],
      "metadata": {
        "id": "04JUEqlKbLvD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ResNet Network Architecture\n",
        "\n",
        "The first ResNet architecture was the Resnet-34, which involved the insertion of shortcut connections in turning a plain network into its residual network counterpart. In this case, the plain network was inspired by VGG neural networks (VGG-16, VGG-19), with the convolutional networks having 3×3 filters. However, compared to VGGNets, ResNets have fewer filters and lower complexity. The 34-layer ResNet achieves a performance of 3.6 bn FLOPs, compared to 1.8bn FLOPs of smaller 18-layer ResNets.\n",
        "\n",
        "It also followed two simple design rules –  the layers had the same number of filters for the same output feature map size, and the number of filters doubled in case the feature map size was halved in order to preserve the time complexity per layer. It consisted of 34 weighted layers.\n",
        "\n",
        "The shortcut connections were added to this plain network. While the input and output dimensions were the same, the identity shortcuts were directly used. With an increase in the dimensions, there were two options to be considered. The first was that the shortcut would still perform identity mapping while extra zero entries would be padded for increasing dimensions. The other option was to use the projection shortcut to match dimensions. Comparison of plain network i ResNet is shown below:\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://i.ibb.co/kGFPw9M/resnet-vs-plain.png\">\n",
        "</p>\n",
        "\n",
        "Various improvements of an original ResNet34 are developed through the years. One example is Resnet50 architecture which introduced one major difference. In the case of ResNet50, the building block was modified into a bottleneck design due to concerns over the time taken to train the layers. This used a stack of 3 layers instead of the earlier 2.\n",
        "\n",
        "Therefore, each of the 2-layer blocks in Resnet34 was replaced with a 3-layer bottleneck block, forming the Resnet 50 architecture. This has much higher accuracy than the 34-layer ResNet model. The 50-layer ResNet achieves a performance of 3.8bn FLOPS!\n",
        "\n"
      ],
      "metadata": {
        "id": "0ESfd2gZbg4W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will implement ResNet34 and ResNet50 for the task of single and multiple label classification. \n",
        "\n",
        "Training will take a lot of time, you may go and grab some coffee while waiting... "
      ],
      "metadata": {
        "id": "Vpp05D0W2OPJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Single lable classification\n",
        "\n",
        "For this task, we will use the [Oxford-IIIT Pet Dataset](https://www.robots.ox.ac.uk/~vgg/data/pets/) that contains images of cats and dogs of 37 different breeds. We will first show how to build a simple cat-vs-dog classifier."
      ],
      "metadata": {
        "id": "lat6pJ3TkXqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#hide\n",
        "#skip\n",
        "! [ -e /content ] && pip install -Uqq fastai  # upgrade fastai on colab"
      ],
      "metadata": {
        "id": "1VAAmxvNk-Rv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fastai.vision.all import *"
      ],
      "metadata": {
        "id": "Bgo5xlpjlF-o"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = untar_data(URLs.PETS)"
      ],
      "metadata": {
        "id": "bXq6W8B8kUix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It will only do this download once, and return the location of the decompressed archive. We can check what is inside with the `.ls()` method."
      ],
      "metadata": {
        "id": "48o73ylxklp0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path.ls()"
      ],
      "metadata": {
        "id": "a4pMI4j_ko7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will ignore the annotations folder for now, and focus on the images one. `get_image_files` is a fastai function that helps us grab all the image files (recursively) in one folder."
      ],
      "metadata": {
        "id": "qNDDpWRskpgf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "files = get_image_files(path/\"images\")\n",
        "len(files)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIcIrXZqktXz",
        "outputId": "85d0df46-db18-4d41-eb54-b92cb28bc400"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7390"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To label our data for the cats vs dogs problem, we need to know which filenames are of dog pictures and which ones are of cat pictures. There is an easy way to distinguish: the name of the file begins with a capital for cats, and a lowercased letter for dogs."
      ],
      "metadata": {
        "id": "5TfEbHF_kzPw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "files[0],files[6]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BU-er0wk1Bb",
        "outputId": "d7ce9bde-633d-49b7-f370-78f20bee5fd2"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Path('/root/.fastai/data/oxford-iiit-pet/images/american_bulldog_196.jpg'),\n",
              " Path('/root/.fastai/data/oxford-iiit-pet/images/american_bulldog_143.jpg'))"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can then define an easy label function:"
      ],
      "metadata": {
        "id": "cYRG1PsdlLx8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def label_func(f): return f[0].isupper()"
      ],
      "metadata": {
        "id": "rMCmDMbhlMjF"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get our data ready for a model, we need to put it in a `DataLoaders` object. Here we have a function that labels using the file names, so we will use `ImageDataLoaders.from_name_func`. There are other factory methods of `ImageDataLoaders` that could be more suitable for your problem, so make sure to check them all in `vision.data`. "
      ],
      "metadata": {
        "id": "C2kItD6clOdj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dls = ImageDataLoaders.from_name_func(path, files, label_func, item_tfms=Resize(224))"
      ],
      "metadata": {
        "id": "1AOrIzn1lQk7"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have passed to this function the directory we're working in, the `files` we grabbed, our `label_func` and one last piece as `item_tfms`: this is a `Transform` applied on all items of our dataset that will resize each image to 224 by 224, by using a random crop on the largest dimension to make it a square, then resizing to 224 by 224. If we didn't pass this, we would get an error later as it would be impossible to batch the items together.\n",
        "\n",
        "We can then check if everything looks okay with the `show_batch` method (`True` is for cat, `False` is for dog):"
      ],
      "metadata": {
        "id": "PRyzw-ZolVMz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dls.show_batch()"
      ],
      "metadata": {
        "id": "wuDFM_hRlYNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we can create a `Learner`, which is a fastai object that combines the data and a model for training, and uses transfer learning to fine tune a pretrained model in just two lines of code:"
      ],
      "metadata": {
        "id": "nfUYavLulY95"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learn = cnn_learner(dls, resnet34, metrics=error_rate)\n",
        "learn.fine_tune(1)"
      ],
      "metadata": {
        "id": "a1NOqUlglbs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first line downloaded a model called ResNet34, pretrained on [ImageNet](http://www.image-net.org/), and adapted it to our specific problem. It then fine tuned that model and in a relatively short time, we get a model with an error rate of 0.3%... amazing!\n",
        "\n",
        "If you want to make a prediction on a new image, you can use `learn.predict`:"
      ],
      "metadata": {
        "id": "Cw-ZZVuWlgR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learn.predict(files[0])"
      ],
      "metadata": {
        "id": "nH1sjzTHlkgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The predict method returns three things: the decoded prediction (here `False` for dog), the index of the predicted class and the tensor of probabilities of all classes in the order of their indexed labels(in this case, the model is quite confifent about the being that of a dog). This method accepts a filename, a PIL image or a tensor directly in this case.\n",
        "We can also have a look at some predictions with the `show_results` method:"
      ],
      "metadata": {
        "id": "sL9p5aSbllTG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learn.show_results()"
      ],
      "metadata": {
        "id": "jxRggEWRlolT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-label classification\n",
        "\n",
        "For this task, we will use the [Pascal Dataset](http://host.robots.ox.ac.uk/pascal/VOC/) that contains images with different kinds of objects/persons. It's orginally a dataset for object detection, meaning the task is not only to detect if there is an instance of one class of an image, but to also draw a bounding box around it. Here we will just try to predict all the classes in one given image.\n",
        "\n",
        "Multi-label classification defers from before in the sense each image does not belong to one category. An image could have a person *and* a horse inside it for instance. Or have none of the categories we study.\n",
        "\n",
        "As before, we can download the dataset pretty easily:"
      ],
      "metadata": {
        "id": "FVOkm3w9ltab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = untar_data(URLs.PASCAL_2007)\n",
        "path.ls()"
      ],
      "metadata": {
        "id": "B48hnwGBn0jM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The information about the labels of each image is in the file named `train.csv`. We load it using pandas:"
      ],
      "metadata": {
        "id": "anzPXf00n-SE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(path/'train.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "S7dvAdrjoAKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, what exactly multi-label classification means? That's pretty straightforward: for each filename, we will get the different labels (separated by space) and the last column tells if it's in the validation set or not. To get this in `DataLoaders` quickly, we have a factory method, `from_df`. We can specify the underlying path where all the images are, an additional folder to add between the base path and the filenames (here `train`), the `valid_col` to consider for the validation set (if we don't specify this, we take a random subset), a `label_delim` to split the labels and, as before, `item_tfms` and `batch_tfms`.\n",
        "\n",
        "Note that we don't have to specify the `fn_col` and the `label_col` because they default to the first and second column respectively."
      ],
      "metadata": {
        "id": "36MU0QnioKAE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dls = ImageDataLoaders.from_df(df, path, folder='train', valid_col='is_valid', label_delim=' ',\n",
        "                               item_tfms=Resize(460), batch_tfms=aug_transforms(size=224))"
      ],
      "metadata": {
        "id": "PSSos4PvoV4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As before, we can then have a look at the data with the `show_batch` method."
      ],
      "metadata": {
        "id": "v4WF5GceoYl1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dls.show_batch()"
      ],
      "metadata": {
        "id": "ROZEU-bmoayZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training a model is as easy (but takes a lot of time, depending on GPU) as before: the same functions can be applied and the fastai library will automatically detect that we are in a multi-label problem, thus picking the right loss function. The only difference is in the metric we pass: `error_rate` will not work for a multi-label problem, but we can use `accuracy_thresh`."
      ],
      "metadata": {
        "id": "FJdXzbiJobf3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learn = cnn_learner(dls, resnet50, metrics=partial(accuracy_multi, thresh=0.5))"
      ],
      "metadata": {
        "id": "9Mm8uI-noqGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As before, we can use `learn.lr_find` to pick a good learning rate:"
      ],
      "metadata": {
        "id": "cVipARwFoqpR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learn.lr_find()"
      ],
      "metadata": {
        "id": "IPYTl178otiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can pick the suggested learning rate and fine-tune our pretrained model:"
      ],
      "metadata": {
        "id": "Efcj4uVeovOh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learn.fine_tune(2, 3e-2)"
      ],
      "metadata": {
        "id": "jO7IXeN6o0f_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Like before, we can easily have a look at the results:"
      ],
      "metadata": {
        "id": "6-szXxIyo2hT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learn.show_results()"
      ],
      "metadata": {
        "id": "84ktuGCVo4n2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Or get the predictions on a given image:"
      ],
      "metadata": {
        "id": "e7V4Vv7ro6uX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learn.predict(path/'train/000005.jpg')"
      ],
      "metadata": {
        "id": "lLeuitgeo8fo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As for the single classification predictions, we get three things. The last one is the prediction of the model on each class (going from 0 to 1). The second to last cooresponds to a one-hot encoded targets (you get `True` for all predicted classes, the ones that get a probability > 0.5) and the first is the decoded, readable version."
      ],
      "metadata": {
        "id": "8HIk4PrQpCGO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "And like before, we can check where the model did its worse:"
      ],
      "metadata": {
        "id": "6CbahOyupEgz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "interp = Interpretation.from_learner(learn)\n",
        "interp.plot_top_losses(9)"
      ],
      "metadata": {
        "id": "ykJNV1_0pF15"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Copy_of_lab1.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
